{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "814ad319-eaec-4815-b743-c5526a3c1708",
   "metadata": {},
   "source": [
    "![](pics/header.jpg)\n",
    "\n",
    "# Neural Network Math\n",
    "\n",
    "Kevin Walchko, Phd\n",
    "\n",
    "---\n",
    "\n",
    "A lot of these notes are taken from Udacity's Artificial Intelligence course."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5cd9343-6c46-4a4f-a61f-c6a486e98249",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Forward Propagation\n",
    "\n",
    "$$\n",
    "\\hat y = \\sigma (Wx+b)\n",
    "$$\n",
    "\n",
    "\n",
    "- $y$: Truth label which is either 0 or 1\n",
    "- $\\hat y$: Prediction or probability on a scale of 0 .. 1, $\\sigma (Wx+b)$\n",
    "- $w_i$: Weight value from previous activation layer perceptron\n",
    "- b: Bias\n",
    "- $x_i$ or $a_i$: activation from a previous perceptron layer, $\\hat y$\n",
    "- perceptron: The perceptron is a simplified model of a biological neuron and another name for activation, $\\hat y$ \n",
    "- $\\sigma$: Activation function. There are many types and this is the simplest. Other examples are ReLU or tanh, $\\sigma = 1/(1+e^{-x})$\n",
    "\n",
    "\n",
    "## Gradient Decent\n",
    "\n",
    "- minimize cost function ($C$), which is the negative of the gradient: $-\\nabla C$ \n",
    "    - $C$ must be continuous and differentiable \n",
    "    - Cost is calculated as: $\\sum_{i=0}^{n-1} (y_i - truth_i)$\n",
    "    - Typically there is a learning rate added ($\\eta$) to control convergence speed: $- \\eta \\nabla C$ \n",
    "    - Step size is a function of slope to minimize overshooting a local minimum\n",
    "- When updating, you adjust your weights and bias based on the sign and magnitude\n",
    "    - $-\\nabla C = \\begin{bmatrix} -1 & -0.3 & 5.6 & -0.001 & \\dots \\end{bmatrix}$ where the -1 tells us to decrease the weights/bias for that output while the +5.6 tells us to increase the wieghts/bias for that output\n",
    "    - Also, the +5.6 is more important (has a greater effect) on the network output than the -0.001\n",
    "    \n",
    "## Cross Entropy\n",
    "\n",
    "- Binary system (red/blue):\n",
    "\n",
    "| | Control | BP Eqn | Def |\n",
    "|---|---|---|---|\n",
    "| activations | N | . | . |\n",
    "| weights     | Y | . | value of weighted connections between perceptrons |\n",
    "| bias        | Y | . | activation threshold |\n",
    "\n",
    "## Maximum Likelihood\n",
    "\n",
    "The probability ($\\hat y$) of something is given by:\n",
    "\n",
    "$$\n",
    "\\hat y = \\sigma (Wx+b)\n",
    "$$\n",
    "\n",
    "## Cross Entropy\n",
    "\n",
    "Since the probability is a value between 0 - 1, the log will be negative, so we correct this by taking the negative, so we get  a positive number. This connects probability and *error function* together.\n",
    "\n",
    "$$\n",
    "\\sum - \\ln(\\hat y)\n",
    "$$\n",
    "\n",
    "- small cross entropy: event and probability are likely\n",
    "- high cross entropy: event and probability are unlikely\n",
    "\n",
    "## Gradient Descent with Squared Errors\n",
    "\n",
    "$$\n",
    "E_{SSE} = \\frac{1}{2}\\sum_{\\mu} \\sum_j \\left[ y^{\\mu}_j - \\hat{y}^{\\mu}_j \\right]^2 \\\\\n",
    "E_{MSE} = \\frac{1}{2m}\\sum_{\\mu} \\left[ y^{\\mu} - \\hat{y}^{\\mu} \\right]^2\n",
    "$$\n",
    "\n",
    "where SSE is the sum of the squared error, MSE is mean of the squared error, $\\hat y$ is the prediction and y is the true value, and you take the sum over all output units j and another sum over all data points $\\mu$. \n",
    "\n",
    "- SSE is always posative\n",
    "- SSE penalizes large errors more than small errors\n",
    "\n",
    "## Gradient Decent\n",
    "\n",
    "$$\n",
    "\\Delta w_{ij} = \\eta \\delta_j x_i \\\\\n",
    "\\delta = (y - \\hat y) f'(h) = (y - \\hat y) f'( \\sum w_i x_i) \\\\\n",
    "h = \\sum w x_i\n",
    "$$\n",
    "\n",
    "where $\\delta$ is the error term, $(y - \\hat y)$ is the output error, and $f'(h)$ refers to the derivative of the activation function, $f(h)$. Now for $f(h)$ we will use the sigmoid.\n",
    "\n",
    "```python\n",
    "# Defining the sigmoid function for activations\n",
    "def sigmoid(x):\n",
    "    return 1/(1+np.exp(-x))\n",
    "\n",
    "# Derivative of the sigmoid function\n",
    "def sigmoid_prime(x):\n",
    "    return sigmoid(x) * (1 - sigmoid(x))\n",
    "\n",
    "# Input data\n",
    "x = np.array([0.1, 0.3])\n",
    "# Target\n",
    "y = 0.2\n",
    "# Input to output weights\n",
    "weights = np.array([-0.8, 0.5])\n",
    "\n",
    "# The learning rate, eta in the weight step equation\n",
    "learnrate = 0.5\n",
    "\n",
    "# the linear combination performed by the node (h in f(h) and f'(h))\n",
    "h = x[0]*weights[0] + x[1]*weights[1]\n",
    "# or h = np.dot(x, weights)\n",
    "\n",
    "# The neural network output (y-hat)\n",
    "nn_output = sigmoid(h)\n",
    "\n",
    "# output error (y - y-hat)\n",
    "error = y - nn_output\n",
    "\n",
    "# output gradient (f'(h))\n",
    "output_grad = sigmoid_prime(h)\n",
    "\n",
    "# error term (lowercase delta)\n",
    "error_term = error * output_grad\n",
    "\n",
    "# Gradient descent step \n",
    "del_w = [ learnrate * error_term * x[0],\n",
    "          learnrate * error_term * x[1]]\n",
    "# or del_w = learnrate * error_term * x\n",
    "```\n",
    "\n",
    "Here's the general algorithm for updating the weights with gradient descent:\n",
    "\n",
    "1. Set the weight step to zero: $\\Delta w_i = 0$\n",
    "1. For each record in the training data:\n",
    "    1. Make a forward pass through the network, calculating the output $\\hat y = f(\\sum_i w_i x_i)$\n",
    "    1. Calculate the error term for the output unit, $\\delta = (y - \\hat y) * f'(\\sum_i w_i x_i)$\n",
    "    1. Update the weight step $\\Delta w_i = \\Delta w_i + \\delta x_i$\n",
    "1. Update the weights $w_i = w_i + \\eta \\Delta w_i / m$ where $\\eta$ is the learning rate and $m$ is the number of records. Here we're averaging the weight steps to help reduce any large variations in the training data.\n",
    "1. Repeat for $e$ epochs.\n",
    "\n",
    "where the activation signal is the sigmoid function $f(h)=1/(1+e^−h)$, $f'(h)=f(h)(1−f(h))$\n",
    "\n",
    "Initialize weights with:\n",
    "\n",
    "```python\n",
    "# Number of records and input units\n",
    "n_records, n_inputs = features.shape\n",
    "# Number of hidden units\n",
    "n_hidden = 2\n",
    "weights_input_to_hidden = np.random.normal(0, n_inputs**-0.5, size=(n_inputs, n_hidden))\n",
    "```\n",
    "\n",
    "## Multi-Layer BP\n",
    "\n",
    "$$\n",
    "\\Delta w_{ij} = \\eta \\delta_j x_i\n",
    "$$\n",
    "\n",
    "where i is for each node in a previous layer and j is for each node of the next layer.\n",
    "\n",
    "output layer: $\\delta_k = (y_k-\\hat y_k)f'(h_k)$\n",
    "hidden layers:\n",
    "\n",
    "**see implementing bp**\n",
    "\n",
    "- [Yes you should understand backprop](https://karpathy.medium.com/yes-you-should-understand-backprop-e2f06eab496b#.vt3ax2kg9)\n",
    "- [CS231n Winter 2016 Lecture 4 Backpropagation, Neural Networks](https://www.youtube.com/watch?v=59Hbtz7XgjM)\n",
    "\n",
    "- Stochastic Gradient Decent: large datasets require lots of computational power and memory to train a NN. The training requirements can be reduced if you break a large dataset up into smaller datasets and train in batches. The performance of each batch won't be as good as the one large dataset, but it will still work faster and with reduced requirements. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b120f3f7-f329-4250-8d64-b5a0f8cb2aa6",
   "metadata": {},
   "source": [
    "## Learning Rate\n",
    "\n",
    "### Momentum\n",
    "\n",
    "- Helps prevent getting stuck in local minimum when doing gradient decent\n",
    "- $\\beta$: momentum value\n",
    "- Weight each step: $step_n = step_n + \\beta step_{n-1} + \\beta^2 step_{n-2} + \\dots$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e64bf8c6-bd39-4f92-9589-0705d43af9b6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
