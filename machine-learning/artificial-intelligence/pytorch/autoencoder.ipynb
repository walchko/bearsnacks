{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2ea16a63-d6e7-46fa-86ee-63fda849ddd3",
   "metadata": {},
   "source": [
    "![](pics/header.png)\n",
    "\n",
    "# Deep Learning: Autoencoder\n",
    "\n",
    "Kevin Walchko\n",
    "\n",
    "---\n",
    "\n",
    "These notes come from Udacity's Deep Learning Nanodegree"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af6395cd-9028-45e1-9cf3-0fb0e95fa670",
   "metadata": {},
   "source": [
    "## Why?\n",
    "\n",
    "Autoencoders are able to reproduce the input. \n",
    "\n",
    "![](pics/autoencoder.png)\n",
    "\n",
    "- Compression: like PCA or dimensional reduction, autoencoder can reduce the input down to the minimum amount of information needed to reconstruct it\n",
    "- Denoising: since the autoencoder understands the minimum set of information (compression above), it can remove unnecessary information (noise) and reproduce the original\n",
    "- Image reconstruction: similar to denoiseing, but instead of random additive values, it works with missing information (damage image or missing color planes) to reconstruct a full, colorized image"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c993d7a-1b3b-4e76-9ab6-ec0f6a5fbfe3",
   "metadata": {},
   "source": [
    "## Error Calculation\n",
    "\n",
    "Mean squared error (MSE) is a good choice when comparing pixel quantities rather than class probabilities. \n",
    "\n",
    "```python\n",
    "criterion = nn.MSELoss()\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d86bd42-7938-4f7a-880e-0a094962b633",
   "metadata": {},
   "source": [
    "## Transpose Convolution\n",
    "\n",
    "Convolution can be thought of as downsampling an image. To up sample an image, you would do transposed convolution which can be thought of as the inverse of regular convolution.\n",
    "\n",
    "![](pics/transposed-conv.gif)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ddb84b8-1baf-444a-a277-bece145dfadc",
   "metadata": {},
   "source": [
    "## Alternative to Transposed Convolution\n",
    "\n",
    "Transposed convolution can leave artifacts in yiur image. Thus, an alternative is to:\n",
    "\n",
    "1. `F.upsample(x)`[deprecated] or `F.interpolate(x)` down/up samples the input to either \n",
    "the given size or the given scale_factor\n",
    "1. `F.relu(nn.Conv2d(x))`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "679814f3-6cef-4a4b-947a-bbb6b7c19a34",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from helper import summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "992b19b7-2616-4a07-98e1-122946912144",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on function interpolate in module torch.nn.functional:\n",
      "\n",
      "interpolate(input: torch.Tensor, size: Optional[int] = None, scale_factor: Optional[List[float]] = None, mode: str = 'nearest', align_corners: Optional[bool] = None, recompute_scale_factor: Optional[bool] = None, antialias: bool = False) -> torch.Tensor\n",
      "    Down/up samples the input to either the given :attr:`size` or the given\n",
      "    :attr:`scale_factor`\n",
      "    \n",
      "    The algorithm used for interpolation is determined by :attr:`mode`.\n",
      "    \n",
      "    Currently temporal, spatial and volumetric sampling are supported, i.e.\n",
      "    expected inputs are 3-D, 4-D or 5-D in shape.\n",
      "    \n",
      "    The input dimensions are interpreted in the form:\n",
      "    `mini-batch x channels x [optional depth] x [optional height] x width`.\n",
      "    \n",
      "    The modes available for resizing are: `nearest`, `linear` (3D-only),\n",
      "    `bilinear`, `bicubic` (4D-only), `trilinear` (5D-only), `area`, `nearest-exact`\n",
      "    \n",
      "    Args:\n",
      "        input (Tensor): the input tensor\n",
      "        size (int or Tuple[int] or Tuple[int, int] or Tuple[int, int, int]):\n",
      "            output spatial size.\n",
      "        scale_factor (float or Tuple[float]): multiplier for spatial size. If `scale_factor` is a tuple,\n",
      "            its length has to match `input.dim()`.\n",
      "        mode (str): algorithm used for upsampling:\n",
      "            ``'nearest'`` | ``'linear'`` | ``'bilinear'`` | ``'bicubic'`` |\n",
      "            ``'trilinear'`` | ``'area'`` | ``'nearest-exact'``. Default: ``'nearest'``\n",
      "        align_corners (bool, optional): Geometrically, we consider the pixels of the\n",
      "            input and output as squares rather than points.\n",
      "            If set to ``True``, the input and output tensors are aligned by the\n",
      "            center points of their corner pixels, preserving the values at the corner pixels.\n",
      "            If set to ``False``, the input and output tensors are aligned by the corner\n",
      "            points of their corner pixels, and the interpolation uses edge value padding\n",
      "            for out-of-boundary values, making this operation *independent* of input size\n",
      "            when :attr:`scale_factor` is kept the same. This only has an effect when :attr:`mode`\n",
      "            is ``'linear'``, ``'bilinear'``, ``'bicubic'`` or ``'trilinear'``.\n",
      "            Default: ``False``\n",
      "        recompute_scale_factor (bool, optional): recompute the scale_factor for use in the\n",
      "            interpolation calculation. If `recompute_scale_factor` is ``True``, then\n",
      "            `scale_factor` must be passed in and `scale_factor` is used to compute the\n",
      "            output `size`. The computed output `size` will be used to infer new scales for\n",
      "            the interpolation. Note that when `scale_factor` is floating-point, it may differ\n",
      "            from the recomputed `scale_factor` due to rounding and precision issues.\n",
      "            If `recompute_scale_factor` is ``False``, then `size` or `scale_factor` will\n",
      "            be used directly for interpolation. Default: ``None``.\n",
      "        antialias (bool, optional): flag to apply anti-aliasing. Default: ``False``. Using anti-alias\n",
      "            option together with ``align_corners=False``, interpolation result would match Pillow\n",
      "            result for downsampling operation. Supported modes: ``'bilinear'``, ``'bicubic'``.\n",
      "    \n",
      "    .. note::\n",
      "        With ``mode='bicubic'``, it's possible to cause overshoot, in other words it can produce\n",
      "        negative values or values greater than 255 for images.\n",
      "        Explicitly call ``result.clamp(min=0, max=255)`` if you want to reduce the overshoot\n",
      "        when displaying the image.\n",
      "    \n",
      "    .. note::\n",
      "        Mode ``mode='nearest-exact'`` matches Scikit-Image and PIL nearest neighbours interpolation\n",
      "        algorithms and fixes known issues with ``mode='nearest'``. This mode is introduced to keep\n",
      "        backward compatibility.\n",
      "        Mode ``mode='nearest'`` matches buggy OpenCV's ``INTER_NEAREST`` interpolation algorithm.\n",
      "    \n",
      "    Note:\n",
      "        This operation may produce nondeterministic gradients when given tensors on a CUDA device. See :doc:`/notes/randomness` for more information.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(F.interpolate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b6476345-3fdf-4f60-a05a-fb01afcc26d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==========================================================================================\n",
      "Layer (type (var_name))                  Kernel Shape              Param #\n",
      "==========================================================================================\n",
      "ConvAutoencoder                          --                        --\n",
      "├─Conv2d (conv1)                         [1, 16, 3, 3]             160\n",
      "├─Conv2d (conv2)                         [16, 4, 3, 3]             580\n",
      "├─MaxPool2d (pool)                       --                        --\n",
      "├─Conv2d (conv4)                         [4, 16, 3, 3]             592\n",
      "├─Conv2d (conv5)                         [16, 1, 3, 3]             145\n",
      "==========================================================================================\n",
      "Total params: 1,477\n",
      "Trainable params: 1,477\n",
      "Non-trainable params: 0\n",
      "==========================================================================================\n"
     ]
    }
   ],
   "source": [
    "# define the NN architecture\n",
    "class ConvAutoencoder(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(ConvAutoencoder, self).__init__()\n",
    "        ## encoder layers ##\n",
    "        # conv layer (depth from 1 --> 16), 3x3 kernels\n",
    "        self.conv1 = nn.Conv2d(1, 16, 3, padding=1)  \n",
    "        # conv layer (depth from 16 --> 8), 3x3 kernels\n",
    "        self.conv2 = nn.Conv2d(16, 4, 3, padding=1)\n",
    "        # pooling layer to reduce x-y dims by two; kernel and stride of 2\n",
    "        self.pool = nn.MaxPool2d(2, 2)\n",
    "        \n",
    "        ## decoder layers ##\n",
    "        self.conv4 = nn.Conv2d(4, 16, 3, padding=1)\n",
    "        self.conv5 = nn.Conv2d(16, 1, 3, padding=1)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # add layer, with relu activation function\n",
    "        # and maxpooling after\n",
    "        x = F.relu(self.conv1(x))\n",
    "        x = self.pool(x)\n",
    "        # add hidden layer, with relu activation function\n",
    "        x = F.relu(self.conv2(x))\n",
    "        x = self.pool(x)  # compressed representation\n",
    "        \n",
    "        ## decoder \n",
    "        # upsample, followed by a conv layer, with relu activation function  \n",
    "        # this function is called `interpolate` in some PyTorch versions\n",
    "        x = F.upsample(x, scale_factor=2, mode='nearest')\n",
    "        x = F.relu(self.conv4(x))\n",
    "        # upsample again, output should have a sigmoid applied\n",
    "        x = F.upsample(x, scale_factor=2, mode='nearest')\n",
    "        x = F.sigmoid(self.conv5(x))\n",
    "        \n",
    "        return x\n",
    "\n",
    "# initialize the NN\n",
    "model = ConvAutoencoder()\n",
    "summary(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84a3b6d9-b7da-4f11-a988-735cada99061",
   "metadata": {},
   "source": [
    "## Style Transfer\n",
    "\n",
    "- Gram matrix (G) contains non-localized information and contains information about the style of a given layer: \n",
    "    - given a block of feature maps (dxhxw), vectorize each feature map: 8x4x4 -> 8x16\n",
    "    - now, multiply by transpose: 8x16 x 16x8 = 8x8 = G\n",
    "    - G(1,2) contains the similarities between layer 1 and layer 2 or feature map 1 and feature map 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71ea3946-1ee9-446b-b891-aa4c14643798",
   "metadata": {},
   "source": [
    "## Style Loss\n",
    "\n",
    "Style loss is calculated from the MSE between the target and style gram matrices. This loss is decreased by <u>only</u> changing the Target image.\n",
    "\n",
    "$$\n",
    "\\mathcal{L}_{style} = a \\sum_i w_i (T_{s,i} - S_{s,i})^2 \\\\\n",
    "\\mathcal{L}_{content} = \\frac {1}{2} \\sum (T_c - C_c)^2 \\\\\n",
    "TotalStyleTransferLoss = \\alpha \\mathcal{L}_{content} + \\beta \\mathcal{L}_{style} \\\\\n",
    "\\alpha < \\beta \\Rightarrow \\frac {\\alpha}{\\beta}\n",
    "$$\n",
    "\n",
    "as $\\frac {\\alpha}{\\beta}$ decreases, there is less content ($\\alpha$) and more style ($\\beta$) in an image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a4cff80-ad9a-4e6e-b1ed-b2dda4876647",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
