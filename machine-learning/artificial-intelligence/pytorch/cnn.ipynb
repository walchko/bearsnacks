{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4868f67e-85af-4500-890a-814b478191b8",
   "metadata": {},
   "source": [
    "![](pics/header.png)\n",
    "\n",
    "# Deep Learning: Convolutional Neural Network (CNN)\n",
    "\n",
    "Kevin Walchko\n",
    "\n",
    "---\n",
    "\n",
    "These notes come from Udacity's Deep Learning Nanodegree"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc101101-b416-4c7d-ab81-16153dc4f2d6",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Convolutional Neural Network (CNN)\n",
    "\n",
    "![](pics/pnas.webp)\n",
    "\n",
    "CNNs can be trained to find features within a region of an image and understand the combination of features to identify an object.\n",
    "\n",
    "Usually, we think the layers do:\n",
    "\n",
    "- First layer finds edges or other primative high frequency features\n",
    "- Second layer finds groups of edges from primative edges\n",
    "- Third finds object identifying features from groups\n",
    "- etc ...\n",
    "\n",
    "*Note:* It is not this simple, but a good starting point for understanding.\n",
    "\n",
    "| Multi-Layer Perceptron (MLP) | Convolutional Neural Network (CNN) |\n",
    "|------------------------------|------------------------------------|\n",
    "| only operate on **vector** inputs, images were flattened first | operate on 2D (matrix) data |\n",
    "| Vector input, no understanding of 2D relationship of pixels | Matrix input, can hardness the spatial relationship of pixels |\n",
    "| Only fully connected layers, larger parameter set to train, long training time | Sparsely connected layers, reduced parameter set to train, faster training time |\n",
    "\n",
    "\n",
    "- **Highpass Filters:**\n",
    "    - sharpen an image\n",
    "    - enhance high frequency parts of an image like an edge\n",
    "    - convlutional edge detection kernel's elements must sum to zero because they are looking for the difference or change between pixels\n",
    "    \n",
    "## CNN Layer\n",
    "\n",
    "The CNN layer can act on each channel of an image or just apply different convolutional kernels to the same grayscale image.\n",
    "\n",
    "<table> \n",
    "    <tr>\n",
    "        <table>\n",
    "            <tr>\n",
    "                <td><img src=\"pics/conv-layer-1.png\"></td>\n",
    "                <td><img src=\"pics/conv-layer-2.png\"></td>\n",
    "            </tr>\n",
    "        </table>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td><img src=\"pics/conv-layer-3.png\"></td>\n",
    "    </tr>\n",
    "</table>\n",
    "\n",
    "### Convolutional Layer\n",
    "\n",
    "```python\n",
    "# create a numpy array of filter values\n",
    "filter_vals = np.array([[-1, -1, 1, 1], [-1, -1, 1, 1], [-1, -1, 1, 1], [-1, -1, 1, 1]])\n",
    "\n",
    "# define four filters from the values above\n",
    "filter_1 = filter_vals\n",
    "filter_2 = -filter_1\n",
    "filter_3 = filter_1.T\n",
    "filter_4 = -filter_3\n",
    "filters = np.array([filter_1, filter_2, filter_3, filter_4])\n",
    "\n",
    "k_channels, k_height, k_width = filters.shape\n",
    "# defines the convolutional layer, assumes there are 4 grayscale filters\n",
    "# torch.nn.Conv2d(in_channels, out_channels, kernel_size, \n",
    "#                 stride=1, padding=0, dilation=1, groups=1, bias=True)\n",
    "self.conv = nn.Conv2d(1, k_channels, kernel_size=(k_height, k_width), bias=False)\n",
    "self.conv.weight = torch.nn.Parameter(filters)\n",
    "```\n",
    "\n",
    "The filters look like:\n",
    "\n",
    "![](pics/cnn-filters.png)\n",
    "\n",
    "Filter layers typically increase in depth (or number of feature detectors) as they go along the pipeline. However, the width and height of the image remains the same *until* a maxpooling layer is encountered to reduce the feature map size.\n",
    "\n",
    "Definitions:\n",
    "\n",
    "- **Channels:** each color plane **OR** feature map produced by a filter\n",
    "    - RGB image would have 3 channels in, but I could apply 2 filters to each input channel for a total of 6 output channels: 3 colors x 2 filters = 6 channels\n",
    "- **Filter:** convolutional filter that detects an image feature\n",
    "- **Stride:** how many pixels you shift the filter over before applying, default is usually 1 pixel\n",
    "- **Padding:** adding extra numbers (column and/or row values, typically set to 0) so the filter has numbers to work with on the edges of the image\n",
    "    - for a kernel of size 7x7, to keep the same size output array as input array, you would need a padding of 3. This is because the center pixel needs 3 more pixels around the center pixel to do the convolution calculation with"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57f1c44a-b161-4db6-8c3c-5d1da7c6a1bc",
   "metadata": {},
   "source": [
    "## Pooling Layers\n",
    "\n",
    "When applying many convolutional filters to an image, you need a way to reduce the dimensionality or you will have <u>too many</u> parameters to train. A Pooling layer will collapse down the result and reduce dimensionality\n",
    "\n",
    "![](pics/maxpooling.png)\n",
    "\n",
    "```python\n",
    "# nn.MaxPool2d(window_size, stride)\n",
    "#\n",
    "# stride does not impact depth\n",
    "#\n",
    "# stride is the factor the input feature map is decimated by\n",
    "# 2 -> 1/2 or 200x200 -> 100x100\n",
    "# 4 -> 1/4 or 200x200 -> 50x50\n",
    "# etc\n",
    "self.pool = nn.MaxPool2d(2, 2) # reduces feature map by 1/2\n",
    "```\n",
    "\n",
    "- **Max pooling layer:**\n",
    "    - window size and stride\n",
    "    - works on all feature maps from an *n* set of convolution filters at once and returns the max value\n",
    "    - Conv(4filters x W x H) -> Pool(window(2 x 2),stride(2)) -> (4filters x W/2 x H/2)\n",
    "- Pooling layers throw away information and can lose spatial understanding\n",
    "- Pooling layers are less popular today and can produce unexpected results\n",
    "    - They can correctly idenitfy a face even when extra eyes have been photoshopped in because there is a loss in spatial understanding between location and amount of eyes a face has"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "935f8b18-2657-4849-a540-e3b3cbf6e17a",
   "metadata": {},
   "source": [
    "## Build a CNN Network\n",
    "\n",
    "Determining parameters and sizes of feature maps in a CNN:\n",
    "\n",
    "- K: out_channels\n",
    "- F: kernel_size\n",
    "- D_in: depth of previous layer, typically 1 (grayscale) or 3 (RGB)\n",
    "- **Parameters to train:** K\\*F\\*F\\*D_in + K\n",
    "- S: the stride of the convolution\n",
    "- P: the padding\n",
    "- W_in: the width/height (square) of the previous layer\n",
    "- **Convolutional layer shape:** (W_inâˆ’F+2P)/S+1\n",
    "\n",
    "Example: color image (3x130x130) as input\n",
    "\n",
    "1. nn.Conv2d(3, 10, 3)\n",
    "    - image: 3x130x130, depth: 10, kernel: 3, padding: 0, stride: 1\n",
    "    - shape: (130-3+2\\*0)/1+1 = 130\n",
    "    - depth: 10\n",
    "1. nn.MaxPool2d(4, 4)\n",
    "    - output shape: 10x32x32\n",
    "1. nn.Conv2d(10, 20, 5, padding=2)\n",
    "    - featureMap: 10x32x32, depth: 20, kernel: 5, padding: 2, stride: 1\n",
    "    - shape: (32-5+2\\*2)/1+1 = 32\n",
    "    - depth: 20\n",
    "1. nn.MaxPool2d(2, 2)\n",
    "    - output shape: 20x16x16\n",
    "\n",
    "```python\n",
    "import torch.nn as nn\n",
    "from torch.nn import functional as F\n",
    "\n",
    "class Network(nn.Model):\n",
    "    def __init__(self, in_channesls, out_channels, kernel_size):\n",
    "        super(Network, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(in_channels, out_channels, kernel_size, stride=1, padding=0)\n",
    "        self.pool = nn.MaxPool2d(2,2)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.conv1(x))\n",
    "        x = self.pool(x)\n",
    "        \n",
    "        return x\n",
    "    \n",
    "# grayscale image 200x200\n",
    "# input channels = 1 (grayscale)\n",
    "# want 16 convolutional filters applied resulting in 200x200x16\n",
    "n = Network(1,16)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a961a30-42ee-4cea-8e96-548b049a5131",
   "metadata": {},
   "source": [
    "## Classifiers and Transfer Learning\n",
    "\n",
    "As the feature map moves through successive layers of filtering and pooling, the details of the pixels matter less. Instead, the network is finding features and will be flattened and feed into a classifier. \n",
    "\n",
    "Now below, the feature detector will identify parts of the car, say tires, windows, etc and then feed this to the classifer portion for it to determine, \"this is a car!\"\n",
    "\n",
    "<img src=\"pics/cnn-classifier.png\" width=\"500px\">\n",
    "\n",
    "Now, you have a couple of options based on data size and how similar your new application is compared to the original application and that is shown below in the pictures. Basically you can: \n",
    "\n",
    "- Take a trained network and remove the classifer for one application and replace it with a custom classifier for another application. You also need to freeze the feature detector weights so you don't change them. The neat thing about this method is you don't have to re-train the feature detector portion of the network.\n",
    "- Take a trained network, replace the classifier at the end. However, you retrain the entire network, but use the pre-trained weights as a starting point.\n",
    "\n",
    "<table> \n",
    "    <tr>\n",
    "        <img src=\"pics/transfer-learning.png\">\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td><img src=\"pics/d.png\"></td>\n",
    "        <td><img src=\"pics/c.png\"></td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td><img src=\"pics/a.png\"></td>\n",
    "        <td><img src=\"pics/b.png\"></td>\n",
    "    </tr>\n",
    "</table>\n",
    "\n",
    "\n",
    "### Freezing Parameters\n",
    "\n",
    "```python\n",
    "for param in vgg.features.parameters():\n",
    "    param.requires_grad = False\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d2b9943-acaf-475f-b2b0-18934fd9bb4c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
