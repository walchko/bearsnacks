{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "af9aaf3e-02af-409d-abc2-3a13e3a2ecd0",
   "metadata": {},
   "source": [
    "![](pics/header.png)\n",
    "\n",
    "# TorchVision Pre-Trained Networks\n",
    "\n",
    "Kevin Walchko\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b64882d-cc06-4444-96f5-49d8078b6bd4",
   "metadata": {},
   "source": [
    "`torchvision` comes with several models you can load.\n",
    "\n",
    "```python\n",
    "model = torchvision.models.vgg16(pretrained=True)\n",
    "```\n",
    "\n",
    "![](pics/summary.png)\n",
    "\n",
    "### VGG\n",
    "\n",
    "VGG takes in a 224x244x3 sized image and outputs a vector of 1000 possible \n",
    "things the image could be. The NN is broken up into 2 parts:\n",
    "\n",
    "- feature detector\n",
    "- classifier called `fc` in the table below. When doing transfer learning, \n",
    "this classifier can be replaced with a different network for a particular \n",
    "application w/o having to train the feature detector part of the NN.\n",
    "\n",
    "![](pics/vgg16-s.png)\n",
    "\n",
    "![](pics/vgg16.png)\n",
    "\n",
    "### Alexnet\n",
    "\n",
    "![](pics/alexnet.png)\n",
    "\n",
    "### GoogLeNet\n",
    "\n",
    "![](pics/googlenet.jpg)\n",
    "\n",
    "### ResNet\n",
    "\n",
    "![](pics/resnet.png)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "83c99ec1-795d-4482-8fdd-c801b1c01278",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchvision\n",
    "from helper import summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "18facfba-cf30-40ce-9fc8-64cf2f31dc81",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['AlexNet', 'ConvNeXt', 'DenseNet', 'EfficientNet', 'GoogLeNet', 'GoogLeNetOutputs', 'Inception3', 'InceptionOutputs', 'MNASNet', 'MobileNetV2', 'MobileNetV3', 'RegNet', 'ResNet', 'ShuffleNetV2', 'SqueezeNet', 'VGG', 'VisionTransformer']\n"
     ]
    }
   ],
   "source": [
    "print([s for s in dir(torchvision.models) if s[0].isupper()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "24a6a2f1-f52c-4c36-890b-ee1123313708",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==========================================================================================\n",
      "Layer (type (var_name))                  Kernel Shape              Param #\n",
      "==========================================================================================\n",
      "VGG                                      --                        --\n",
      "├─Sequential (features)                  --                        --\n",
      "│    └─Conv2d (0)                        [3, 64, 3, 3]             1,792\n",
      "│    └─ReLU (1)                          --                        --\n",
      "│    └─Conv2d (2)                        [64, 64, 3, 3]            36,928\n",
      "│    └─ReLU (3)                          --                        --\n",
      "│    └─MaxPool2d (4)                     --                        --\n",
      "│    └─Conv2d (5)                        [64, 128, 3, 3]           73,856\n",
      "│    └─ReLU (6)                          --                        --\n",
      "│    └─Conv2d (7)                        [128, 128, 3, 3]          147,584\n",
      "│    └─ReLU (8)                          --                        --\n",
      "│    └─MaxPool2d (9)                     --                        --\n",
      "│    └─Conv2d (10)                       [128, 256, 3, 3]          295,168\n",
      "│    └─ReLU (11)                         --                        --\n",
      "│    └─Conv2d (12)                       [256, 256, 3, 3]          590,080\n",
      "│    └─ReLU (13)                         --                        --\n",
      "│    └─Conv2d (14)                       [256, 256, 3, 3]          590,080\n",
      "│    └─ReLU (15)                         --                        --\n",
      "│    └─MaxPool2d (16)                    --                        --\n",
      "│    └─Conv2d (17)                       [256, 512, 3, 3]          1,180,160\n",
      "│    └─ReLU (18)                         --                        --\n",
      "│    └─Conv2d (19)                       [512, 512, 3, 3]          2,359,808\n",
      "│    └─ReLU (20)                         --                        --\n",
      "│    └─Conv2d (21)                       [512, 512, 3, 3]          2,359,808\n",
      "│    └─ReLU (22)                         --                        --\n",
      "│    └─MaxPool2d (23)                    --                        --\n",
      "│    └─Conv2d (24)                       [512, 512, 3, 3]          2,359,808\n",
      "│    └─ReLU (25)                         --                        --\n",
      "│    └─Conv2d (26)                       [512, 512, 3, 3]          2,359,808\n",
      "│    └─ReLU (27)                         --                        --\n",
      "│    └─Conv2d (28)                       [512, 512, 3, 3]          2,359,808\n",
      "│    └─ReLU (29)                         --                        --\n",
      "│    └─MaxPool2d (30)                    --                        --\n",
      "├─AdaptiveAvgPool2d (avgpool)            --                        --\n",
      "├─Sequential (classifier)                --                        --\n",
      "│    └─Linear (0)                        [25088, 4096]             102,764,544\n",
      "│    └─ReLU (1)                          --                        --\n",
      "│    └─Dropout (2)                       --                        --\n",
      "│    └─Linear (3)                        [4096, 4096]              16,781,312\n",
      "│    └─ReLU (4)                          --                        --\n",
      "│    └─Dropout (5)                       --                        --\n",
      "│    └─Linear (6)                        [4096, 1000]              4,097,000\n",
      "==========================================================================================\n",
      "Total params: 138,357,544\n",
      "Trainable params: 138,357,544\n",
      "Non-trainable params: 0\n",
      "==========================================================================================\n"
     ]
    }
   ],
   "source": [
    "vgg16 = torchvision.models.vgg16(pretrained=True)\n",
    "summary(vgg16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "7b1a4b1f-6288-4ce8-a80c-49f38c9a3712",
   "metadata": {},
   "outputs": [],
   "source": [
    "# help(torchvision.models.VGG)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "169d6a95-465b-4a19-87e7-97a6686eaed3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
