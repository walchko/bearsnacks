{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1b308c52-ea7c-468b-94a8-6fcc95855da3",
   "metadata": {},
   "source": [
    "![](pics/header.png)\n",
    "\n",
    "# Deep Learning: Long Short Term Memory (LSTM)\n",
    "\n",
    "Kevin Walchko\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b62b45c2-e62b-4806-9205-98e17ffac3a0",
   "metadata": {},
   "source": [
    "## Overview\n",
    "\n",
    "<table> \n",
    "    <tr>\n",
    "        <td><img src=\"pics/lstm/rnn-neuron.png\" width=\"70%\"></td>\n",
    "        <td><img src=\"pics/lstm/lstm.png\"></td>\n",
    "    </tr>\n",
    "</table>\n",
    "\n",
    "A normal RNN neuron is shown on the left and an LSTM cell is shown on the right.\n",
    "\n",
    "- In general, LSTMs and GRUs perform better than vanilla RNNs\n",
    "- Avoid vanshing gradient problem\n",
    "- Learns over many time steps with backpropagation\n",
    "    - Fully differentiable\n",
    "    - Contains: sigmoid, hyperbolic tangent, multiplications and addition\n",
    "    \n",
    "Limitations of RNNs\n",
    "\n",
    "- Encoding bottleneck\n",
    "- Slow, no parallelization\n",
    "- Not long term memory\n",
    "    \n",
    "## References\n",
    "\n",
    "- CS231n: [Andrej Karpathy's lecture on RNN and LSTMs](https://www.youtube.com/watch?v=iX5V1WpxxkY)\n",
    "- Chris Olah's Understanding LSTM Networks [post](http://colah.github.io/posts/2015-08-Understanding-LSTMs/)\n",
    "- Edwin Chen's LSTM [post](http://blog.echen.me/2017/05/30/exploring-lstms/)\n",
    "- Visualizing Machine Learning [blog](https://jalammar.github.io/)\n",
    "- PyTorch docs: [LSTM](https://pytorch.org/docs/stable/generated/torch.nn.LSTM.html#torch.nn.LSTM)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a961f991-1569-48bd-a7be-57d06f94eec7",
   "metadata": {},
   "source": [
    "## LSTM\n",
    "\n",
    "![](pics/lstm/lstm-eq.jpg)\n",
    "\n",
    "where $x_t$ is the event, $C_t$ is long term memory and $h_t$ is short term memory."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a24f58e7-c8bf-413a-af1c-9e8ebfbe79c9",
   "metadata": {},
   "source": [
    "## Gated Recurrent Units (GRUs)\n",
    "\n",
    "A gated recurrent unit (GRU) is basically an LSTM without an output gate, which therefore fully writes the contents from its memory cell to the larger net at each time step."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7431d24-7251-4aa0-a087-14934bd87b52",
   "metadata": {},
   "source": [
    "## Embeddings\n",
    "\n",
    "Embedding allows NNs to learn from text by converting them to numbers. It also has the effect of reducing the dimensionality.\n",
    "\n",
    "- Code words to number: \"heart\" = 981\n",
    "- Input the word code to a fully connected embedding layer which transcribes it to a kind of one hot encoding\n",
    "    - When dealing with millions of words, one hot encoding is inefficient since almost a million of your values are 0 and only one is 1\n",
    "    - Basically it is like a lookup table that already did the all of the multiplications and basically now all you do is (in the heart example above) grab the 981st row of the weight table\n",
    "        - You do this because everything else is multiplied by 0 and that row is the only one multiplied by 1 ... no real multiplication ... faster!\n",
    "- The embedding layer then feeds the output to the reset of the network\n",
    "\n",
    "### Reference\n",
    "\n",
    "- Efficient Estimation of Wrod Representations in Vector Space [pdf](https://video.udacity-data.com/topher/2018/October/5bc56d28_word2vec-mikolov/word2vec-mikolov.pdf)\n",
    "- Distributed Representations of Wrods and Phrases and their Compositionality [pdf](https://video.udacity-data.com/topher/2018/October/5bc56da8_distributed-representations-mikolov2/distributed-representations-mikolov2.pdf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2ff655a8-c7df-4a0a-a215-f1252fdbd392",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from helper import summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3abd5eeb-3fb2-4396-9ca8-d3ae3db108d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==========================================================================================\n",
      "Layer (type (var_name))                  Kernel Shape              Param #\n",
      "==========================================================================================\n",
      "LSTM (LSTM)                              --                        5,920\n",
      "==========================================================================================\n",
      "Total params: 5,920\n",
      "Trainable params: 5,920\n",
      "Non-trainable params: 0\n",
      "==========================================================================================\n"
     ]
    }
   ],
   "source": [
    "lstm = nn.LSTM(\n",
    "        input_size=10, \n",
    "        hidden_size=20, \n",
    "        num_layers=2, \n",
    "        dropout=0.2, \n",
    "        batch_first=True)\n",
    "\n",
    "summary(lstm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "e0abdb6d-f3f3-45ad-b69b-dce41db8d663",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([5, 3, 20])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "h0 - initial hidden state\n",
    "c0 - inital cell state\n",
    "\"\"\"\n",
    "input = torch.randn(5, 3, 10) # (batch_size, sequence_len, input_size)\n",
    "h0 = torch.randn(2, 5, 20)    # (num_layers, batch_size, hidden_size)\n",
    "c0 = torch.randn(2, 5, 20)    # (num_layers, batch_size, hidden_size)\n",
    "output, (hn, cn) = lstm(input, (h0, c0))\n",
    "output.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "105c64be-0ef6-4f06-9503-29a2218f31ab",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
