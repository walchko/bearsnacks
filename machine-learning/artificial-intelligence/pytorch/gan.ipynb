{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bbacae03-e332-40b2-b447-6e7ea7f2d2ca",
   "metadata": {},
   "source": [
    "![](pics/header.png)\n",
    "\n",
    "# Deep Learning: Generative Adversarial Network (GAN)\n",
    "\n",
    "Kevin Walchko\n",
    "\n",
    "---\n",
    "\n",
    "## References and Examples\n",
    "\n",
    "- [CartoonGAN](https://video.udacity-data.com/topher/2018/November/5bea23cd_cartoongan/cartoongan.pdf)\n",
    "- [StackGAN: Text to Photo-realistic Image Synthesis with Stacked Generative Adversarial Networks](https://arxiv.org/abs/1612.03242)\n",
    "- [Generative Adversarial Nets](https://arxiv.org/pdf/1406.2661.pdf)\n",
    "- DCGAN: [UNSUPERVISED REPRESENTATION LEARNING WITH DEEP CONVOLUTIONAL GENERATIVE ADVERSARIAL NETWORKS](https://arxiv.org/pdf/1511.06434.pdf)\n",
    "- DCGAN: [Batch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift](https://arxiv.org/pdf/1502.03167.pdf)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e50c038-1af3-49b4-bb19-1058ee18fa12",
   "metadata": {},
   "source": [
    "## Overview\n",
    "\n",
    "<img src=\"pics/gan/gan2.png\" width=\"50%\">\n",
    "\n",
    "- Creates new output based on training from realworld data\n",
    "    - Generator: creates output based on random noise input from probability distributions from real data\n",
    "    - Descriminator: tries to determine the probability the input data is real or fake\n",
    "- Since the Generator is trying to fool the Discriminator, it will constanty change its output to move up hill along the function learned by the Discriminator until the probabily the data the Discriminator sees is 50% (real or fake)\n",
    "    - In the picture above, see how the distribution on the left eventually moves on top of the real data distribution on the right?\n",
    "- GANs use game theory\n",
    "- Run 2 optimizers, one for G and one for D\n",
    "    - Adam is a good optimizer for both\n",
    "    - Use [BCEWithLogitsLoss](https://pytorch.org/docs/stable/generated/torch.nn.BCEWithLogitsLoss.html#torch.nn.BCEWithLogitsLoss) since you are just determining fake (0) or real (1)\n",
    "        - d_loss = nn.BCEWithLogitsLoss(logits, labels*0.9)\n",
    "        - g_loss = nn.BCEWithLogitsLoss(logits, flipped_labels)\n",
    "        - Image -> D -> logits -> sigmoid -> probabilities\n",
    "        - This combines a sigmoid activation function and and binary cross entropy loss in one function.\n",
    "    - All hidden layers will have [Leaky ReLu](https://pytorch.org/docs/stable/nn.html#torch.nn.LeakyReLU) applied to their output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37dbbebe-2e63-4d50-a656-344c02e1cc26",
   "metadata": {},
   "source": [
    "## Architecture\n",
    "\n",
    "<img src=\"pics/gan/gan_network.png\" width=\"50%\">\n",
    "\n",
    "> **Note:** You can do the sigmoid in the loss function if you want. Here we will use the `nn.BCEWithLogitsLoss`. This loss combines a `Sigmoid` layer and the `BCELoss` in one single class. This version is more numerically stable than using a plain `Sigmoid` followed by a `BCELoss` as, by combining the operations into one layer, we take advantage of the log-sum-exp trick for numerical stability.\n",
    "\n",
    "```python\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "class Discriminator(nn.Module):\n",
    "\n",
    "    def __init__(self, input_size, hidden_dim, output_size):\n",
    "        super(Discriminator, self).__init__()\n",
    "        \n",
    "        # define hidden linear layers\n",
    "        self.fc1 = nn.Linear(input_size, hidden_dim*4)\n",
    "        self.fc2 = nn.Linear(hidden_dim*4, hidden_dim*2)\n",
    "        self.fc3 = nn.Linear(hidden_dim*2, hidden_dim)\n",
    "        \n",
    "        # final fully-connected layer\n",
    "        self.fc4 = nn.Linear(hidden_dim, output_size)\n",
    "        \n",
    "        # dropout layer \n",
    "        self.dropout = nn.Dropout(0.3)\n",
    "        \n",
    "        \n",
    "    def forward(self, x):\n",
    "        # flatten image\n",
    "        x = x.view(-1, 28*28)\n",
    "        # all hidden layers\n",
    "        x = F.leaky_relu(self.fc1(x), 0.2) # (input, negative_slope=0.2)\n",
    "        x = self.dropout(x)\n",
    "        x = F.leaky_relu(self.fc2(x), 0.2)\n",
    "        x = self.dropout(x)\n",
    "        x = F.leaky_relu(self.fc3(x), 0.2)\n",
    "        x = self.dropout(x)\n",
    "        # final layer\n",
    "        out = self.fc4(x)\n",
    "\n",
    "        return x\n",
    "    \n",
    "class Generator(nn.Module):\n",
    "\n",
    "    def __init__(self, input_size, hidden_dim, output_size):\n",
    "        super(Generator, self).__init__()\n",
    "        \n",
    "        # define hidden linear layers\n",
    "        self.fc1 = nn.Linear(input_size, hidden_dim)\n",
    "        self.fc2 = nn.Linear(hidden_dim, hidden_dim*2)\n",
    "        self.fc3 = nn.Linear(hidden_dim*2, hidden_dim*4)\n",
    "        \n",
    "        # final fully-connected layer\n",
    "        self.fc4 = nn.Linear(hidden_dim*4, output_size)\n",
    "        \n",
    "        # dropout layer \n",
    "        self.dropout = nn.Dropout(0.3)\n",
    "        \n",
    "\n",
    "    def forward(self, x):\n",
    "        # all hidden layers\n",
    "        x = F.leaky_relu(self.fc1(x), 0.2) # (input, negative_slope=0.2)\n",
    "        x = self.dropout(x)\n",
    "        x = F.leaky_relu(self.fc2(x), 0.2)\n",
    "        x = self.dropout(x)\n",
    "        x = F.leaky_relu(self.fc3(x), 0.2)\n",
    "        x = self.dropout(x)\n",
    "        # final layer with tanh applied\n",
    "        out = F.tanh(self.fc4(x)) # sigmoid in nn.BCEWithLogitsLoss\n",
    "        \n",
    "        return x\n",
    "\n",
    "# Calculate losses\n",
    "def real_loss(D_out, smooth=False):\n",
    "    batch_size = D_out.size(0)\n",
    "    # label smoothing\n",
    "    if smooth:\n",
    "        # smooth, real labels = 0.9\n",
    "        labels = torch.ones(batch_size)*0.9\n",
    "    else:\n",
    "        labels = torch.ones(batch_size) # real labels = 1\n",
    "        \n",
    "    # numerically stable loss\n",
    "    criterion = nn.BCEWithLogitsLoss()\n",
    "    # calculate loss\n",
    "    loss = criterion(D_out.squeeze(), labels)\n",
    "    return loss\n",
    "\n",
    "def fake_loss(D_out):\n",
    "    batch_size = D_out.size(0)\n",
    "    labels = torch.zeros(batch_size) # fake labels = 0\n",
    "    criterion = nn.BCEWithLogitsLoss()\n",
    "    # calculate loss\n",
    "    loss = criterion(D_out.squeeze(), labels)\n",
    "    return loss\n",
    "\n",
    "\n",
    "# learning rate for optimizers\n",
    "lr = 0.002\n",
    "\n",
    "# Create optimizers for the discriminator and generator\n",
    "d_optimizer = optim.Adam(D.parameters(), lr)\n",
    "g_optimizer = optim.Adam(G.parameters(), lr)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a4e48ee-aea2-409f-aa25-1da00d904295",
   "metadata": {},
   "source": [
    "## Deep Convolutional GAN (DCGAN)\n",
    "\n",
    "<table> \n",
    "    <tr>\n",
    "        <td><img src=\"pics/gan/dcgan-g.png\" width=\"691px\"></td>\n",
    "        <td><img src=\"pics/gan/dcgan-d.png\" width=\"691px\"></td>\n",
    "    </tr>\n",
    "</table>\n",
    "\n",
    "> Architecture guidelines for stable Deep Convolutional GANs:\n",
    ">\n",
    "> - Replace any pooling layers with strided convolutions (discriminator) and fractional-strided convolutions (generator).\n",
    "> - Use batchnorm (mean=0, variance=1) in both the generator and the discriminator.\n",
    "> - Remove fully connected hidden layers for deeper architectures.\n",
    "> - Use ReLU activation in generator for all layers except for the output, which uses Tanh.\n",
    "> - Use LeakyReLU activation in the discriminator for all layers."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de6a33cd-0075-4e2d-8050-f0a011ad728d",
   "metadata": {},
   "source": [
    "## Batch Normalization\n",
    "\n",
    "Batch normalization normalizes the output of a previous layer by subtracting the batch mean and dividing by the batch standard deviation. This results in:\n",
    "\n",
    "- **Networks train faster:** Each training iteration will actually be slower because of the extra calculations during the forward pass and the additional hyperparameters to train during back propagation. However, it should converge much more quickly, so training should be faster overall.\n",
    "- **Allows higher learning rates:** Gradient descent usually requires small learning rates for the network to converge. And as networks get deeper, their gradients get smaller during back propagation so they require even more iterations. Using batch normalization allows us to use much higher learning rates, which further increases the speed at which networks train.\n",
    "- **Makes weights easier to initialize:** Weight initialization can be difficult, and it's even more difficult when creating deeper networks. Batch normalization seems to allow us to be much less careful about choosing our initial starting weights.\n",
    "- **Makes more activation functions viable:** Some activation functions do not work well in some situations. Sigmoids lose their gradient pretty quickly, which means they can't be used in deep networks. And ReLUs often die out during training, where they stop learning completely, so we need to be careful about the range of values fed into them. Because batch normalization regulates the values going into each activation function, non-linearlities that don't seem to work well in deep networks actually become viable again.\n",
    "- **Simplifies the creation of deeper networks:** Because of the first 4 items listed above, it is easier to build and faster to train deeper neural networks when using batch normalization. And it's been shown that deeper networks generally produce better results, so that's great.\n",
    "- **Provides a bit of regularization:** Batch normalization adds a little noise to your network. In some cases, such as in Inception modules, batch normalization has been shown to work as well as dropout. But in general, consider batch normalization as a bit of extra regularization, possibly allowing you to reduce some of the dropout you might add to a network.\n",
    "- **May give better results overall:** Some tests seem to show batch normalization actually improves the training results. However, it's really an optimization to help train faster, so you shouldn't think of it as a way to make your network better. But since it lets you train networks faster, that means you can iterate over more designs more quickly. It also lets you build deeper networks, which are usually better. So when you factor in everything, you're probably going to end up with better results if you build your networks with batch normalization\n",
    "\n",
    "Basic equations:\n",
    "\n",
    "$$\n",
    "\\mu_B \\leftarrow \\frac {1}{m} \\sum^m_{i=1} x_i \\\\\n",
    "\\sigma^2_B \\leftarrow \\frac {1}{m} \\sum^m_{i=1} (x_i - \\mu_B)^2 \\\\\n",
    "\\hat x_i \\leftarrow \\frac {x_i - \\mu_B} {\\sqrt{\\sigma^2_B + \\epsilon}} \\\\\n",
    "y_i \\leftarrow \\gamma \\hat x_i + \\beta\n",
    "$$\n",
    "\n",
    "where $\\epsilon$ is any small positive value (e.g., 0.001) to ensure we don't divide by zero, $\\hat x_i$ is the normalized value, and both $\\gamma$ and $\\beta$ are learnable parameters that modify the normalized value before it ($y_i$) is feed into the next layer.\n",
    "\n",
    "## PyTorch Batch Normalization\n",
    "\n",
    "- Layers with batch normalization do not include a bias term. So, for linear or convolutional layers, you'll need to set bias=False if you plan to add batch normalization on the outputs.\n",
    "- You can use PyTorch's [nn.BatchNorm1d](https://pytorch.org/docs/stable/generated/torch.nn.BatchNorm1d.html#torch.nn.BatchNorm1d) function to handle the math on linear outputs or [nn.BatchNorm2d](https://pytorch.org/docs/stable/generated/torch.nn.BatchNorm2d.html#torch.nn.BatchNorm2d) for 2D outputs, like filtered images from convolutional layers.\n",
    "- You add the batch normalization layer before calling the activation function, so it always goes layer -> batch norm -> activation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f90007a-9290-434e-87b0-f8e970d8da8c",
   "metadata": {},
   "source": [
    "## Pix2Pix and CycleGAN\n",
    "\n",
    "## Loss Function\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fdcca63-7b49-47e0-a9fb-7fe153edc24b",
   "metadata": {},
   "source": [
    "## Logits\n",
    "\n",
    "The vector of raw (non-normalized) predictions that a classification model generates, which is ordinarily then passed to a normalization function. If the model is solving a multi-class classification problem, logits typically become an input to the softmax function. The softmax function then generates a vector of (normalized) probabilities with one value for each possible class. \n",
    "\n",
    "- stack overflow:[ref](https://stackoverflow.com/a/60543547/5374768) \n",
    "- pytorch: [ref](https://developers.google.com/machine-learning/glossary#logits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "401005ba-0d2d-4048-822d-1ab1b29a0424",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6de7096d-602c-4194-9f2b-3b1dee3ba515",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Conv2d\n",
      "'NoneType' object has no attribute 'data'\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    c = nn.Conv2d(32,1,3,bias=False)\n",
    "    print(c.__class__.__name__)\n",
    "    print(c.bias.data)\n",
    "except Exception as e:\n",
    "    print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "13d95224-7d6e-4594-afb2-2cbde3d1ec17",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.0078])\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    cc = nn.Conv2d(32,1,3)\n",
    "    print(cc.bias.data)\n",
    "except Exception as e:\n",
    "    print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "962dbc2d-b7c6-4aeb-bff2-1b6f6aa86d11",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
