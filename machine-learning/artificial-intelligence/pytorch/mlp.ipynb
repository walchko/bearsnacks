{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f0174d10-ff71-429e-b315-ec89823796c1",
   "metadata": {},
   "source": [
    "![](pics/header.png)\n",
    "\n",
    "# Deep Learning: Multi-Layer Perceptron (MLP) Networks\n",
    "\n",
    "Kevin Walchko\n",
    "\n",
    "---\n",
    "\n",
    "These notes come from Udacity's Deep Learning Nanodegree"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbeddf58-df2c-4cf5-a347-2306d147c582",
   "metadata": {},
   "source": [
    "## Resources\n",
    "\n",
    "- github: [udacity deep-learning-v2-pytorch](https://github.com/udacity/deep-learning-v2-pytorch)\n",
    "- github: [deeptraffic](https://github.com/lexfridman/deeptraffic) **broken**\n",
    "- github: [flappybird](https://github.com/yenchenlin/DeepLearningFlappyBird)\n",
    "- [Neural Networks and Deep Learning](http://neuralnetworksanddeeplearning.com/chap1.html)\n",
    "- [Deep Learning](https://www.deeplearningbook.org/)\n",
    "- [CNN driving car in Grand Theft Auto](https://pythonprogramming.net/game-frames-open-cv-python-plays-gta-v/)\n",
    "- [CS231n Convolutional Neural Networks for Visual Recognition](https://cs231n.github.io/understanding-cnn/)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79c23e27-60e9-4758-90a8-e55eca2e06b8",
   "metadata": {},
   "source": [
    "## Signal to Noise\n",
    "\n",
    "The neural net is a tool to find a pattern. However, if there is too much noise in the training data, then it can be hard or impossible for it to do that. If your inputs are [18, 200, 0, 0.1, 2 ...], then a noise term that is multiplied by the 200 term could make training difficult.\n",
    "\n",
    "So either scale your inputs such that they are 0 or 1 or in the continous range of [0,1]."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94082553-bf30-4597-b004-0f5dc84d65db",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Setting Initial Weights\n",
    "\n",
    "Setting initial weights, you have a couple options:\n",
    "\n",
    "- **constant values:** set initial weights to 0 or 1, however, if all of the weights are the same in your network, the backpropagation step has a difficult time determining the gradient and performs poorly.\n",
    "- **normal distribution:** pick random values in `np.random.normal(mean, std, size)` where `mean` is 0, `std` is 1.0 and size is a function of layer inputs/outputs. **This is generally your best option** and is basically the default solution for `torch`.\n",
    "- **uniform distribution:** see the discussion below, but normal distribution is slightly better with large networks. For small networks, there is little difference between normal and uniform distributions.\n",
    "\n",
    "The general rule for setting the weights in a neural network is to set them to be close to zero without being too small. A good value is [-y, y] where:\n",
    "\n",
    "$$\n",
    "y = \\frac {1}{\\sqrt{n}}\n",
    "$$\n",
    "\n",
    "where $n$ is the number of inputs to the layer. Making this change *should* enable training the NN with a larger learning rate!\n",
    "\n",
    "```python\n",
    "# Initialize weights\n",
    "\n",
    "# These are the weights between the input layer and the hidden layer.\n",
    "self.weights_0_1 = np.zeros((self.input_nodes,self.hidden_nodes))\n",
    "\n",
    "# These are the weights between the hidden layer and the output layer.\n",
    "## NOTE: the difference in the standard deviation of the normal weights\n",
    "## This was changed from `self.output_nodes**-0.5` to `self.hidden_nodes**-0.5`\n",
    "self.weights_1_2 = np.random.normal(\n",
    "    0.0, # mean \n",
    "    self.hidden_nodes**-0.5, # standard deviation \n",
    "    (self.hidden_nodes, self.output_nodes)) # size of random weight matrix\n",
    "\n",
    "```\n",
    "\n",
    "*Note:* Here, the hidden nodes are the input since this code removed the input layer to reduce the memory footprint of the NN.\n",
    "\n",
    "> Apply those weights to an initialized model using `nn.Model.apply(fn)`, which applies a function to each model layer.\n",
    "\n",
    "```python\n",
    "# this uses Torch functions rather than Numpy functions as seen above\n",
    "\n",
    "# normal distribution\n",
    "def weights_init_normal(m):\n",
    "    classname = m.__class__.__name__\n",
    "    # for every Linear layer in a model..\n",
    "    if classname.find('Linear') != -1:\n",
    "        # get the number of the inputs\n",
    "        n = m.in_features\n",
    "        y = (1.0/np.sqrt(n))\n",
    "        m.weight.data.normal_(0, y)\n",
    "        m.bias.data.fill_(0)\n",
    "        \n",
    "# uniform distribution\n",
    "def weights_init_uniform_rule(m):\n",
    "    classname = m.__class__.__name__\n",
    "    # for every Linear layer in a model..\n",
    "    if classname.find('Linear') != -1:\n",
    "        # get the number of the inputs\n",
    "        n = m.in_features\n",
    "        y = 1.0/np.sqrt(n)\n",
    "        m.weight.data.uniform_(-y, y)\n",
    "        m.bias.data.fill_(0)\n",
    "\n",
    "# create a new model with these weights\n",
    "model_rule = Net()\n",
    "model_rule.apply(weights_init_uniform_rule)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec17be82-7360-4def-80f1-03f3504e0753",
   "metadata": {},
   "source": [
    "## Multi-Layer Perceptron\n",
    "\n",
    "Generally you use CNN for image work, but the MNIST data set is simple enough and well conditioned enough that a MLP NN can be used.\n",
    "\n",
    "- **MNIST Data:** grayscale of handwritten numbers that are normalized (0-1) and 28 x 28 pixels in size\n",
    "    - This dataset is preprocessed and very clean, therefore, it will work nicely with MLP\n",
    "    - If this data was messier, then you would need to use a CNN\n",
    "    - Other [classifiers](http://yann.lecun.com/exdb/mnist/) on the MNIST dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b694f76-1e0c-45ff-899b-bfcf887970c8",
   "metadata": {
    "tags": []
   },
   "source": [
    "## MLP Development Pipeline\n",
    "\n",
    "![](pics/pipeline.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f53365d6-0321-4093-abe4-4af08ea05950",
   "metadata": {},
   "source": [
    "## Using PyTorch\n",
    "\n",
    "<table> \n",
    "    <tr>\n",
    "        <td><img src=\"pics/neuron.png\"></td>\n",
    "        <td><img src=\"pics/model.jpg\"></td>\n",
    "    </tr>\n",
    "</table>\n",
    "\n",
    "- $LayerOutput = f(xW+b)$ where: \n",
    "    - $f$ is an activation function\n",
    "    - $x$ is an input from a previous layer\n",
    "    - $W$ are the weights applied between neuron layers\n",
    "    - $b$ is a bias applied to the network\n",
    "- [Activation functions](https://cs231n.github.io/neural-networks-1/#actfun)\n",
    "- Final outputs of a NN are one of two things:\n",
    "    - **Class scores:** output of a NN with higher positive being the answer\n",
    "    - **Class probability:** output of a NN is a probability, usually from `LogSoftmax(class_score)`\n",
    "\n",
    "## Build the Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3865ccec-7111-47a8-a712-985293b901dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "from torch.nn import functional as F\n",
    "from helper import summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c7424c93-b2f5-49b0-806c-9e12d32efe11",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Define the NN architecture\n",
    "class Net(nn.Module):\n",
    "    def __init__(self, dp=0.2):\n",
    "        super(Net, self).__init__()\n",
    "        \n",
    "        # linear layer (784 inputs -> 128 hidden -> 64 hidden -> 10 output)\n",
    "        self.fc1 = nn.Linear(28 * 28, 128)\n",
    "        self.fc2 = nn.Linear(128, 64)\n",
    "        self.fc3 = nn.Linear(64, 10)\n",
    "        \n",
    "        # dropout prevents overfitting\n",
    "        self.dropout = nn.Dropout(dp)\n",
    "        \n",
    "        # optional\n",
    "        # self.log_softmax = nn.LogSoftmax(dim=1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # flatten image input\n",
    "        x = x.view(-1, 28 * 28)\n",
    "        \n",
    "        # NN\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.dropout(x)\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.dropout(x)\n",
    "        \n",
    "        # get class scores as output\n",
    "        x = self.fc3(x)\n",
    "        # or to get probabilities as output\n",
    "        # x = self.log_softmax(self.fc3(x))\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "53f801d2-b746-4817-ac64-ca10e9892b9d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Net(\n",
      "  (fc1): Linear(in_features=784, out_features=128, bias=True)\n",
      "  (fc2): Linear(in_features=128, out_features=64, bias=True)\n",
      "  (fc3): Linear(in_features=64, out_features=10, bias=True)\n",
      "  (dropout): Dropout(p=0.2, inplace=False)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "model = Net()\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2590945e-2714-4b5f-b7aa-7dc393fb8c1b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==========================================================================================\n",
      "Layer (type (var_name))                  Kernel Shape              Param #\n",
      "==========================================================================================\n",
      "Net                                      --                        --\n",
      "├─Linear (fc1)                           [784, 128]                100,480\n",
      "├─Linear (fc2)                           [128, 64]                 8,256\n",
      "├─Linear (fc3)                           [64, 10]                  650\n",
      "├─Dropout (dropout)                      --                        --\n",
      "==========================================================================================\n",
      "Total params: 109,386\n",
      "Trainable params: 109,386\n",
      "Non-trainable params: 0\n",
      "==========================================================================================\n"
     ]
    }
   ],
   "source": [
    "summary(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c23eaa3-1714-4302-abf8-db9fe8c94da7",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Train the Network\n",
    "\n",
    "The steps for training/learning from a batch of data:\n",
    "\n",
    "1. Clear the gradients of all optimized variables\n",
    "2. Forward pass: compute predicted outputs by passing inputs to the model\n",
    "3. Calculate the loss\n",
    "4. Backward pass: compute gradient of the loss with respect to model parameters\n",
    "5. Perform a single optimization step (parameter update)\n",
    "6. Update average training loss\n",
    "\n",
    "- Training NN with data;\n",
    "    - Training data: used to update weights during backpropagation\n",
    "    - Validation data: used to check how well the model generalizes **but not used to update weights**\n",
    "- Testing data: \n",
    "    - Not seen during training but used to test accuracy of **trained** model\n",
    "    - This helps prevent over fitting\n",
    "\n",
    "```python\n",
    "# specify loss function\n",
    "# criterion = nn.NLLLoss()\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# specify optimizer\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "# optimizer = optim.SGD(model.parameters(), lr=0.003)\n",
    "\n",
    "###################\n",
    "# train the model #\n",
    "###################\n",
    "for data, target in train_loader:\n",
    "    # clear the gradients of all optimized variables\n",
    "    optimizer.zero_grad()\n",
    "    # forward pass: compute predicted outputs by passing inputs to the model\n",
    "    output = model(data)\n",
    "    # calculate the loss\n",
    "    loss = criterion(output, target)\n",
    "    # backward pass: compute gradient of the loss with respect to model parameters\n",
    "    loss.backward()\n",
    "    # perform a single optimization step (parameter update)\n",
    "    optimizer.step()\n",
    "    # update running training loss\n",
    "    train_loss += loss.item()*data.size(0)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7761ee66-a9d6-431b-be40-a31b0ad9248e",
   "metadata": {},
   "source": [
    "## Test the Network\n",
    "\n",
    "![](pics/mlp-mnist.png)\n",
    "\n",
    "```python\n",
    "# initialize lists to monitor test loss and accuracy\n",
    "test_loss = 0.0\n",
    "class_correct = list(0. for i in range(10))\n",
    "class_total = list(0. for i in range(10))\n",
    "\n",
    "model.eval() # prep model for *evaluation*\n",
    "\n",
    "for data, target in test_loader:\n",
    "    # forward pass: compute predicted outputs by passing inputs to the model\n",
    "    output = model(data)\n",
    "    # calculate the loss\n",
    "    loss = criterion(output, target)\n",
    "    # update test loss \n",
    "    test_loss += loss.item()*data.size(0)\n",
    "    # convert output probabilities to predicted class\n",
    "    _, pred = torch.max(output, 1)\n",
    "    # compare predictions to true label\n",
    "    correct = np.squeeze(pred.eq(target.data.view_as(pred)))\n",
    "    # calculate test accuracy for each object class\n",
    "    \n",
    "    for label, c in zip(target.data, correct):\n",
    "        class_correct[label] += c.item()\n",
    "        class_total[label] += 1\n",
    "\n",
    "# calculate and print avg test loss\n",
    "test_loss = test_loss/len(test_loader.dataset)\n",
    "print('Test Loss: {:.6f}\\n'.format(test_loss))\n",
    "\n",
    "for i in range(10):\n",
    "    if class_total[i] > 0:\n",
    "        print('Test Accuracy of %5s: %2d%% (%2d/%2d)' % (\n",
    "            str(i), 100 * class_correct[i] / class_total[i],\n",
    "            class_correct[i], class_total[i]))\n",
    "    else:\n",
    "        print('Test Accuracy of %5s: N/A (no training examples)' % (classes[i]))\n",
    "\n",
    "print('\\nTest Accuracy (Overall): %2d%% (%2d/%2d)' % (\n",
    "    100. * np.sum(class_correct) / np.sum(class_total),\n",
    "    np.sum(class_correct), np.sum(class_total)))\n",
    "```"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
