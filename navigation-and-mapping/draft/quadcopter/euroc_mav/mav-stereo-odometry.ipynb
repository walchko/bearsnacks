{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](https://images.pexels.com/photos/1601217/pexels-photo-1601217.jpeg?auto=compress&cs=tinysrgb&dpr=2&h=750&w=1260)\n",
    "\n",
    "# Euroc MAV\n",
    "\n",
    "Kevin J. Walchko, Phd\n",
    "\n",
    "19 Apr 2020\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2          # opencv itself\n",
    "import numpy as np  # matrix manipulations\n",
    "\n",
    "# to play an mpeg4 video, you can do this:\n",
    "from IPython.display import HTML # need this for embedding a movie in an iframe\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "import pykitti  # this is a modified version of the original, I don't load the lidar data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sensor Layout\n",
    "\n",
    "<img src=\"setup.png\" width=\"85%\">\n",
    "\n",
    "## Video\n",
    "\n",
    "As shown in the diagram above, there are two sets of cameras: Point Grey Flea 2 (FL-14S3M-C) 1.4 Megapixel grayscale (0 and 1) and Point Grey Flea 2 (FL-14S3C-C) 1.4 Megapixel color (2, 3) cameras. I am just working with the grayscale, rectified, uncompressed (lossless png is what they say).\n",
    "\n",
    "```bash\n",
    "- dataset\n",
    "    |\n",
    "    + ----------------------+\n",
    "    |-sequences/            |- poses/\n",
    "       |-00/                   |-00.txt\n",
    "       |  |-calib.txt\n",
    "       |  |-times.txt\n",
    "       |\n",
    "       | ...\n",
    "       |\n",
    "       |-NN/\n",
    "       |   |-calib.txt\n",
    "       |   |-times.txt\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = pykitti.odometry('../do_not_backup/dataset', '00', frames=range(300), lidar=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# these are all PIL images\n",
    "tmp = list(dataset.gray)\n",
    "# convert to OpenCV\n",
    "imgs = [(np.array(x[0]), np.array(x[1]),) for x in tmp]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(imgs[0][0], cmap='gray')\n",
    "type(imgs[0][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# simple function to save a video\n",
    "import platform\n",
    "def videoWrite(frames, fname='out.mp4', fps=30):\n",
    "    frame_height, frame_width = frames[0].shape[:2]\n",
    "    \n",
    "    # the encode hates grayscale, convert to RGB\n",
    "    if len(frames[0].shape) == 2:\n",
    "        convert = True\n",
    "    else:\n",
    "        convert = False\n",
    "    \n",
    "    # pick a good encoder for the current OS\n",
    "    sys = platform.system()\n",
    "    if sys in ['Darwin']:\n",
    "        fourcc = 'avc1'\n",
    "    else:\n",
    "        fourcc = 'mjpg'\n",
    "        \n",
    "    out = cv2.VideoWriter(\n",
    "        fname,\n",
    "        cv2.VideoWriter_fourcc(*fourcc), \n",
    "        fps, \n",
    "        (frame_width,frame_height))\n",
    "    for frame in frames:\n",
    "        if convert:\n",
    "            frame = cv2.cvtColor(frame, cv2.COLOR_GRAY2RGB)\n",
    "        out.write(frame)\n",
    "    out.release()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the left camera images to video\n",
    "left = [x[0] for x in imgs]\n",
    "videoWrite(left, 'drive.mp4', fps=10)\n",
    "print(len(imgs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def videoshow(filename, width=None):\n",
    "    \"\"\"\n",
    "    Display a video in a notebook cell\n",
    "    \"\"\"\n",
    "    import io\n",
    "    import base64\n",
    "    from IPython.display import HTML\n",
    "    \n",
    "    video = io.open(filename, \"rb\").read()\n",
    "    video = base64.b64encode(video)\n",
    "    \n",
    "    if width:\n",
    "        fmt = '<video width={} controls src=\"data:video/mp4;base64,{}\" type=\"video/mp4\" />'\n",
    "        video_tag = fmt.format(width, video.decode(\"ascii\"))\n",
    "    else:\n",
    "        fmt = '<video controls src=\"data:video/mp4;base64,{}\" type=\"video/mp4\" />'\n",
    "        video_tag = fmt.format(video.decode(\"ascii\"))\n",
    "    return HTML(video_tag)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# seems file is too big or something??? Won't load from here, but works if you view it outside jupyter\n",
    "videoshow('drive.mp4', \"100%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ground Truth\n",
    "\n",
    "Where did we really go?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stamps = dataset.timestamps[:len(imgs)]\n",
    "print(len(stamps))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tmp = dataset.poses\n",
    "tp = [(x[0][3],x[2][3],x[1][3],) for x in tmp]\n",
    "len(tp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tp = getTruth('./dataset/poses/00.txt', len(stamps))\n",
    "tmp\n",
    "tx = [x[0] for x in tp]\n",
    "ty = [x[1] for x in tp]\n",
    "tz = [x[2] for x in tp]\n",
    "\n",
    "print(len(stamps), len(tx), len(ty))\n",
    "print(stamps[0])\n",
    "print(tp[0])\n",
    "\n",
    "plt.figure()\n",
    "plt.plot(tx, ty)\n",
    "plt.grid(True)\n",
    "plt.axis('equal')\n",
    "plt.title(\"True Position (x,y) [m]\")\n",
    "\n",
    "plt.figure()\n",
    "plt.plot(tz)\n",
    "plt.grid(True)\n",
    "plt.title(\"True Position (z) [m]\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Video Odometry\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# rotation matrix for camera 0 relative to global?\n",
    "# see calib.txt in dataset/sequences/00\n",
    "def getK():\n",
    "    return   np.array([[7.188560000000e+02, 0, 6.071928000000e+02],\n",
    "              [0, 7.188560000000e+02, 1.852157000000e+02],\n",
    "              [0, 0, 1]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is just a collection of different feature detectors\n",
    "def FAST(im):\n",
    "    fast = cv2.FastFeatureDetector_create()\n",
    "    fast.setNonmaxSuppression(True)\n",
    "    fast.setThreshold(5)  # 20\n",
    "    # find and draw the keypoints\n",
    "    keypoints = fast.detect(im)\n",
    "    keypoints=np.array([[k.pt] for k in keypoints],dtype='f4')  # int?\n",
    "    \n",
    "    \n",
    "    # keypoints = sorted(kp1, key = lambda x:x.response, reverse=True)[:numCorners]\n",
    "    # keypoints = np.array([k.pt for k in keypoints], dtype='int')\n",
    "    \n",
    "    return keypoints\n",
    "\n",
    "def ORB(im):\n",
    "    # this seems to loose points quickly\n",
    "    orb = cv2.ORB_create()\n",
    "    keypoints = orb.detect(im,None)\n",
    "    keypoints=np.array([[k.pt] for k in keypoints],dtype='f4')\n",
    "    return keypoints\n",
    "\n",
    "def ShiTomasi(im):\n",
    "    # params for ShiTomasi corner detection\n",
    "    feature_params = dict( maxCorners = 500,\n",
    "        qualityLevel = 0.3,\n",
    "        minDistance = 7,\n",
    "        blockSize = 7 )\n",
    "    keypoints = cv2.goodFeaturesToTrack(im, mask = None, **feature_params)\n",
    "    return keypoints\n",
    "\n",
    "def featureDetection(im, method=0):\n",
    "    \"\"\"\n",
    "    This initializes the feature to track OR finds new features when the \n",
    "    current group of tracked features falls off the image plane (or said\n",
    "    anther way, are no longer in the image).\n",
    "    \"\"\"\n",
    "    method = 0\n",
    "    keypoints = None\n",
    "    if method == 0:\n",
    "        keypoints = FAST(im)\n",
    "    elif method == 1:\n",
    "        keypoints = ORB(im)\n",
    "    elif method == 2:\n",
    "        keypoints = ShiTomasi(im)\n",
    "    else:\n",
    "        print('ERROR: no method selected for feature detection')\n",
    "\n",
    "    return keypoints"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def featureTrack(new_gray,old_gray,p0):\n",
    "    \"\"\"\n",
    "    Given some initial feature (p0) find the new location of those \n",
    "    features in the new image. All images are grayscale.\n",
    "    \"\"\"\n",
    "    # Parameters for lucas kanade optical flow\n",
    "    lk_params = dict(\n",
    "        winSize  = (10,10),\n",
    "        maxLevel = 3,\n",
    "        criteria = (cv2.TERM_CRITERIA_EPS | cv2.TERM_CRITERIA_COUNT, 10, 0.03)\n",
    "    )\n",
    "\n",
    "    p1, st, err = cv2.calcOpticalFlowPyrLK(old_gray, new_gray, p0, None, **lk_params)\n",
    "\n",
    "    # good points have st==1, so weed out the bad\n",
    "    new = p0[st==1]\n",
    "    old = p1[st==1]\n",
    "    \n",
    "    p1 = np.array([[k] for k in new],dtype=np.float32)\n",
    "    p0 = np.array([[k] for k in old],dtype=np.float32)\n",
    "\n",
    "    return p0, p1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def stereo_match_feature(left_img, right_img, patch_radius, keypoints, min_disp, max_disp):\n",
    "    # in case you want to find stereo match by yourself\n",
    "    h, w = left_img.shape\n",
    "    num_points = keypoints.shape[0]\n",
    "\n",
    "    # Depth (or disparity) map\n",
    "    depth = np.zeros(left_img.shape, np.uint8)\n",
    "    output = np.zeros(keypoints.shape, dtype='int')\n",
    "    all_index = np.zeros((keypoints.shape[0],1), dtype='int').reshape(-1)\n",
    "\n",
    "    r     = patch_radius\n",
    "    # patch_size = 2*patch_radius + 1;\n",
    "\n",
    "    for i in range(num_points):\n",
    "\n",
    "        row, col = keypoints[i,0], keypoints[i,1]\n",
    "        # print(row, col)\n",
    "        best_offset = 0;\n",
    "        best_score = float('inf');\n",
    "\n",
    "        if (row-r < 0 or row + r >= h or col - r < 0 or col + r >= w): continue\n",
    "\n",
    "        left_patch = left_img[(row-r):(row+r+1), (col-r):(col+r+1)] # left imag patch\n",
    "\n",
    "        all_index[i] = 1\n",
    "\n",
    "        for offset in range(min_disp, max_disp+1):\n",
    "\n",
    "              if (row-r) < 0 or row + r >= h or  (col-r-offset) < 0 or (col+r-offset) >= w: continue\n",
    "\n",
    "              diff  = left_patch - right_img[(row-r):(row+r+1), (col-r-offset):(col+r-offset+1)]\n",
    "              sum_s = np.sum(diff**2)\n",
    "\n",
    "              if sum_s < best_score:\n",
    "                  best_score = sum_s\n",
    "                  best_offset = offset\n",
    "\n",
    "        output[i,0], output[i,1] = row,col-best_offset\n",
    "\n",
    "    return output, all_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def removeDuplicate(queryPoints, refPoints, radius=5):\n",
    "    #remove duplicate points from new query points,\n",
    "    for i in range(len(queryPoints)):\n",
    "        query = queryPoints[i]\n",
    "        xliml, xlimh = query[0]-radius, query[0]+radius\n",
    "        yliml, ylimh = query[1]-radius, query[1]+radius\n",
    "        inside_x_lim_mask = (refPoints[:,0] > xliml) & (refPoints[:,0] < xlimh)\n",
    "        curr_kps_in_x_lim = refPoints[inside_x_lim_mask]\n",
    "\n",
    "        if curr_kps_in_x_lim.shape[0] != 0:\n",
    "            inside_y_lim_mask = (curr_kps_in_x_lim[:,1] > yliml) & (curr_kps_in_x_lim[:,1] < ylimh)\n",
    "            curr_kps_in_x_lim_and_y_lim = curr_kps_in_x_lim[inside_y_lim_mask,:]\n",
    "            if curr_kps_in_x_lim_and_y_lim.shape[0] != 0:\n",
    "                queryPoints[i] =  np.array([0,0])\n",
    "    return (queryPoints[:, 0]  != 0 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_keypoints_surf(img1, img2, K, baseline, refPoints = None):\n",
    "    detector = cv2.xfeatures2d.SURF_create(400)\n",
    "    kp1, desc1 = detector.detectAndCompute(img1, None)\n",
    "    kp2, desc2 = detector.detectAndCompute(img2, None)\n",
    "\n",
    "    # FLANN parameters\n",
    "    FLANN_INDEX_KDTREE = 1\n",
    "    index_params = dict(algorithm = FLANN_INDEX_KDTREE, trees = 5)\n",
    "    search_params = dict()   # or pass empty dictionary\n",
    "    flann = cv2.FlannBasedMatcher(index_params,search_params)\n",
    "    matches = flann.knnMatch(desc1,desc2,k=2)\n",
    "\n",
    "    # ratio test as per Lowe's paper\n",
    "    match_points1, match_points2 = [], []\n",
    "    for i,(m,n) in enumerate(matches):\n",
    "        if m.distance < 0.7*n.distance:\n",
    "            match_points1.append(kp1[m.queryIdx].pt)\n",
    "            match_points2.append(kp2[m.trainIdx].pt)\n",
    "\n",
    "    p1 = np.array(match_points1).astype(float)\n",
    "    p2 = np.array(match_points2).astype(float)\n",
    "\n",
    "    if refPoints is not None:\n",
    "        mask = removeDuplicate(p1, refPoints)\n",
    "        p1 = p1[mask,:]\n",
    "        p2 = p2[mask,:]\n",
    "\n",
    "    # 3x4 homogenous matrix for left/right camera\n",
    "    M_left = K.dot(np.hstack((np.eye(3), np.zeros((3,1)))))\n",
    "    M_rght = K.dot(np.hstack((np.eye(3), np.array([[-baseline,0, 0]]).T)))\n",
    "    # rearrange to make function work\n",
    "    p1_flip = np.vstack((p1.T,np.ones((1,p1.shape[0]))))\n",
    "    p2_flip = np.vstack((p2.T,np.ones((1,p2.shape[0]))))\n",
    "    # reconstruct 3d points from camera matrices and points, returns 4xN pts (homogenous)\n",
    "    P = cv2.triangulatePoints(M_left, M_rght, p1_flip[:2], p2_flip[:2])\n",
    "    # these are 4d points, convert to 3d by dividing by the last element\n",
    "    P = P/P[3]\n",
    "    pts_3d = P[:3]\n",
    "\n",
    "    return pts_3d.T, p1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def featureTracking(img_1, img_2, p1, world_points):\n",
    "    # use KLT tracker\n",
    "    lk_params = dict( winSize  = (21,21),\n",
    "                      maxLevel = 3,\n",
    "                      criteria = (cv2.TERM_CRITERIA_EPS | cv2.TERM_CRITERIA_COUNT, 30, 0.01))\n",
    "\n",
    "    p2, st, err = cv2.calcOpticalFlowPyrLK(img_1, img_2, p1, None, **lk_params)\n",
    "    st = st.reshape(st.shape[0])\n",
    "    # find good one\n",
    "    pre = p1[st==1]\n",
    "    p2 = p2[st==1]\n",
    "    w_points = world_points[st==1]  # ??????\n",
    "\n",
    "    return w_points, pre,p2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def playImageSequence(images, K, baseline):\n",
    "    '''\n",
    "        different ways to initialize the query points and landmark points\n",
    "        you can specify the keypoints and landmarks\n",
    "        or you can inilize_3D with FAST corner points, then stere match and \n",
    "        then generate 3D points, but not so accurate\n",
    "        or you can use the OPENCV feature extraction and matching functions\n",
    "    '''\n",
    "    \n",
    "    ret_pos = []\n",
    "    left_img, right_img = images[0]\n",
    "\n",
    "    points, p1 = extract_keypoints_surf(left_img, right_img, K, baseline)\n",
    "    p1 = p1.astype('float32')\n",
    "\n",
    "    pnp_objP = np.expand_dims(points, axis = 2)\n",
    "    pnp_p1   = np.expand_dims(p1, axis = 2).astype(float)\n",
    "\n",
    "    # reference\n",
    "    reference_img = left_img\n",
    "    reference_2D  = p1\n",
    "    landmark_3D   = points\n",
    "\n",
    "    # _, rvec, tvec = cv2.solvePnP(pnp_objP, pnp_p1, K, None)\n",
    "\n",
    "    for i, (left_img, right_img) in enumerate(images):\n",
    "        if i % 20 == 0:\n",
    "            print('image: ', i)\n",
    "\n",
    "        # track points through left image\n",
    "        landmark_3D, reference_2D, tracked_2Dpoints = featureTracking(\n",
    "            reference_img, \n",
    "            left_img, # curImage, \n",
    "            reference_2D,  \n",
    "            landmark_3D)\n",
    "        # print(len(landmark_3D), len(valid_land_mark))\n",
    "\n",
    "        pnp_objP = np.expand_dims(landmark_3D, axis = 2)\n",
    "        pnp_cur  = np.expand_dims(tracked_2Dpoints, axis = 2).astype(float)\n",
    "\n",
    "        # try to estimate object pose given current image points\n",
    "        _, rvec, tvec, inliers = cv2.solvePnPRansac(pnp_objP , pnp_cur, K, None)\n",
    "\n",
    "        # update the new reference_2D\n",
    "        reference_2D = tracked_2Dpoints[inliers[:,0],:]\n",
    "        landmark_3D  = landmark_3D[inliers[:,0],:]\n",
    "\n",
    "        # retrieve the rotation matrix\n",
    "        rot,_ = cv2.Rodrigues(rvec)\n",
    "        tvec = -rot.T.dot(tvec)     # coordinate transformation, from camera to world\n",
    "\n",
    "        inv_transform = np.hstack((rot.T,tvec)) # inverse transform\n",
    "\n",
    "        inliers_ratio = len(inliers)/len(pnp_objP) # the inlier ratio\n",
    "#         print('inliers ratio: ',inliers_ratio)\n",
    "\n",
    "        # re-obtain the 3 D points if the conditions satisfied\n",
    "        # calculate distance tracked keypoints have moved\n",
    "        if (inliers_ratio < 0.9 or len(reference_2D) < 50):\n",
    "            # initiliazation new landmarks\n",
    "            \n",
    "            # landmark_3D, reference_2D = initiliazatize_3D_points(curImage, \n",
    "            #   curImage_R, K, baseline)\n",
    "            # reference_2D = np.fliplr(reference_2D).astype('float32')\n",
    "            \n",
    "            landmark_3D_new, reference_2D_new = extract_keypoints_surf(\n",
    "                left_img,   #curImage, \n",
    "                right_img,  # curImage_R, \n",
    "                K, \n",
    "                baseline, \n",
    "                reference_2D)\n",
    "            # ???\n",
    "            reference_2D_new = reference_2D_new.astype('float32')\n",
    "            landmark_3D_new = inv_transform.dot(\n",
    "                np.vstack((landmark_3D_new.T, np.ones((1,landmark_3D_new.shape[0]))))\n",
    "            )\n",
    "            valid_matches = landmark_3D_new[2,:] >0\n",
    "            landmark_3D_new = landmark_3D_new[:,valid_matches]\n",
    "\n",
    "            reference_2D = np.vstack((reference_2D, reference_2D_new[valid_matches,:]))\n",
    "            landmark_3D =  np.vstack((landmark_3D, landmark_3D_new.T))\n",
    "#         else:\n",
    "#             print(\"*** failed conditions ***\")\n",
    "\n",
    "        reference_img = left_img\n",
    "        ret_pos.append((tvec[0], tvec[2]))\n",
    "    \n",
    "    return ret_pos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run Odometry\n",
    "\n",
    "Now this isn't fast and takes some time. Ideally you should probably code this in C++ for improved speed once you get the algorithm working here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "k = getK()\n",
    "baseline = 0.54\n",
    "pts = playImageSequence(imgs, k, baseline)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cx = [x[0] for x in pts]\n",
    "cy = [x[1] for x in pts]\n",
    "plt.plot(cx, cy, label='Odometry')\n",
    "\n",
    "\n",
    "tx = [x[0] for x in tp[:len(cx)]]\n",
    "ty = [x[1] for x in tp[:len(cx)]]\n",
    "tz = [x[2] for x in tp[:len(cx)]]\n",
    "plt.plot(tx,ty, label='Truth')\n",
    "\n",
    "plt.title('Results for pts[0 - {}]'.format(len(cx)))\n",
    "plt.grid(True)\n",
    "plt.xlabel('x [m]')\n",
    "plt.ylabel('y [m]')\n",
    "plt.axis('equal')\n",
    "plt.legend();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from math import sqrt\n",
    "def calcError(calc, true):\n",
    "    max_err = 0\n",
    "    ret = []\n",
    "    for (cx, cy), (tx, ty, tz) in zip(calc, true):\n",
    "        err = sqrt((tx - cx)**2 + (ty - cy)**2)\n",
    "        ret.append(err)\n",
    "        max_err = err if err > max_err else max_err\n",
    "    return max_err, ret"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_err, err = calcError(pts, tp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dist = sum([sqrt(x[0]**2 + x[1]**2) for x in pts])\n",
    "plt.plot(err)\n",
    "plt.grid(True)\n",
    "plt.title('Odometry Error, Max Error: {:.1f}m over {:.1f}km'.format(max_err, dist/1000));"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
